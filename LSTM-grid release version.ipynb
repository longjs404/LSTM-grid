{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e807b62",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#def_pytorch\" data-toc-modified-id=\"def_pytorch-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>def_pytorch</a></span></li><li><span><a href=\"#ModelSet\" data-toc-modified-id=\"ModelSet-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>ModelSet</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#rescale\" data-toc-modified-id=\"rescale-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>rescale</a></span></li><li><span><a href=\"#single_main\" data-toc-modified-id=\"single_main-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>single_main</a></span></li><li><span><a href=\"#single_func\" data-toc-modified-id=\"single_func-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>single_func</a></span></li><li><span><a href=\"#apply_ufunc---Change\" data-toc-modified-id=\"apply_ufunc---Change-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>apply_ufunc - Change</a></span></li><li><span><a href=\"#model_id\" data-toc-modified-id=\"model_id-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>model_id</a></span></li><li><span><a href=\"#all_main\" data-toc-modified-id=\"all_main-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>all_main</a></span></li><li><span><a href=\"#simple-analyze\" data-toc-modified-id=\"simple-analyze-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>simple analyze</a></span></li><li><span><a href=\"#defective-run\" data-toc-modified-id=\"defective-run-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>defective run</a></span><ul class=\"toc-item\"><li><span><a href=\"#ModelSet_de\" data-toc-modified-id=\"ModelSet_de-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>ModelSet_de</a></span></li><li><span><a href=\"#single_main_de\" data-toc-modified-id=\"single_main_de-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>single_main_de</a></span></li><li><span><a href=\"#single_func_de\" data-toc-modified-id=\"single_func_de-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;</span>single_func_de</a></span></li><li><span><a href=\"#apply_ufunc_de\" data-toc-modified-id=\"apply_ufunc_de-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;</span>apply_ufunc_de</a></span></li><li><span><a href=\"#model_id\" data-toc-modified-id=\"model_id-11.5\"><span class=\"toc-item-num\">11.5&nbsp;&nbsp;</span>model_id</a></span></li><li><span><a href=\"#all_main\" data-toc-modified-id=\"all_main-11.6\"><span class=\"toc-item-num\">11.6&nbsp;&nbsp;</span>all_main</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf74297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:11:28.645827Z",
     "start_time": "2023-05-15T09:10:53.756187Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.nn import Module, LSTM, Linear\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import hydroeval as he\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8338453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-26T15:00:51.169325Z",
     "start_time": "2022-12-26T15:00:51.126045Z"
    }
   },
   "source": [
    "Using the server's graphics card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08ecf0",
   "metadata": {},
   "source": [
    "# def_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1872f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:20.634137Z",
     "start_time": "2023-05-15T09:13:20.474357Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self, config):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = LSTM(input_size=config.input_size, hidden_size=config.hidden_size,\n",
    "                         num_layers=config.lstm_layers, batch_first=True, dropout=config.dropout_rate)  # Define hidden layer\n",
    "        self.linear = Linear(in_features=config.hidden_size, out_features=config.output_size)           # Define fully connected layer\n",
    "\n",
    "    def forward(self, x, hidden=None):                # forward propagation function\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        return linear_out, hidden\n",
    "\n",
    "\n",
    "def train(config, train_and_valid_data):\n",
    "    if config.do_train_visualized:\n",
    "        import visdom\n",
    "        vis = visdom.Visdom(env='def_pytorch')     # Whether to visualize\n",
    "\n",
    "        \n",
    "    global model_id\n",
    "    model_name = 'model' + '_' + str(model_id) + '.pth'  # .pt and .pth are conventional formats\n",
    "    \n",
    "    train_X, train_Y, valid_X, valid_Y = train_and_valid_data\n",
    "    train_X, train_Y = torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).float()  # 转为Tensor\n",
    "    train_loader = DataLoader(TensorDataset(train_X, train_Y),\n",
    "                              batch_size=config.batch_size)  # Generate trainable batch data\n",
    "\n",
    "    valid_X, valid_Y = torch.from_numpy(valid_X).float(), torch.from_numpy(valid_Y).float()\n",
    "    valid_loader = DataLoader(TensorDataset(valid_X, valid_Y), batch_size=config.batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if config.use_cuda and torch.cuda.is_available() else \"cpu\")  # Decide whether to train on CPU or GPU\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)   # Define optimizer\n",
    "    criterion = torch.nn.MSELoss()  # Define loss\n",
    "\n",
    "    valid_loss_min = float(\"inf\")   # Define the initial positive infinity quantity\n",
    "    bad_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    \n",
    "#     print(train_X, train_Y)\n",
    "    \n",
    "    for epoch in range(config.epoch):\n",
    "        model.train()  # Switch to training mode\n",
    "        train_loss_array = []\n",
    "        hidden_train = None\n",
    "        for i, _data in enumerate(train_loader):\n",
    "            _train_X, _train_Y = _data[0].to(device), _data[1].to(device)\n",
    "            optimizer.zero_grad()  # Set gradient information to 0 before training\n",
    "            pred_Y, hidden_train = model(_train_X, hidden_train)  # forward calculation\n",
    "           \n",
    "            hidden_train = None  # If training is non-continuous, reset hidden\n",
    "\n",
    "            loss = criterion(pred_Y, _train_Y)  # Calculate loss\n",
    "            loss.backward()  # Backpropagate loss\n",
    "            optimizer.step()  # Update parameters with optimizer\n",
    "            train_loss_array.append(loss.item())\n",
    "            global_step += 1\n",
    "            if config.do_train_visualized and global_step % 100 == 0:  # Displayed every 100 steps\n",
    "                vis.line(X=np.array([global_step]), Y=np.array([loss.item()]), win='Train_Loss',\n",
    "                         update='append' if global_step > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "\n",
    "        # The following is an early stopping mechanism. When the model training does not improve \n",
    "        # the prediction effect of the validation set for consecutive config.patience epochs, it will stop to prevent overfitting.\n",
    "        model.eval()  # Convert to prediction mode\n",
    "        valid_loss_array = []\n",
    "        hidden_valid = None\n",
    "        for _valid_X, _valid_Y in valid_loader:\n",
    "            _valid_X, _valid_Y = _valid_X.to(device), _valid_Y.to(device)\n",
    "            pred_Y, hidden_valid = model(_valid_X, hidden_valid)\n",
    "            hidden_valid = None\n",
    "            loss = criterion(pred_Y, _valid_Y)  # The verification process only has forward calculation and no backpropagation process.\n",
    "            valid_loss_array.append(loss.item())\n",
    "\n",
    "        train_loss_cur = np.mean(train_loss_array)\n",
    "        valid_loss_cur = np.mean(valid_loss_array)\n",
    "#         print('epoch = ',epoch,' , train loss = ',train_loss_cur,' , valid loss = ',valid_loss_cur)\n",
    "\n",
    "        if config.do_train_visualized:  # The first train_loss_cur is too large and is not displayed in visdom.\n",
    "            vis.line(X=np.array([epoch]), Y=np.array([train_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "            vis.line(X=np.array([epoch]), Y=np.array([valid_loss_cur]), win='Epoch_Loss',\n",
    "                     update='append' if epoch > 0 else None, name='Eval', opts=dict(showlegend=True))\n",
    "\n",
    "        if valid_loss_cur < valid_loss_min:    # Initial value is positive infinity\n",
    "            valid_loss_min = valid_loss_cur    # gradually shrink\n",
    "            bad_epoch = 0\n",
    "            torch.save(model.state_dict(), config.model_save_path + model_name)  # Model save\n",
    "            if valid_loss_cur < 1e-5:\n",
    "                break\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= config.patience:  # If the validation set index does not improve for consecutive epochs, \n",
    "                                              # the training will be stopped.\n",
    "                break\n",
    "#     print('save model as ',model_name) \n",
    "\n",
    "\n",
    "def predict(config, test_X):\n",
    "    \n",
    "    global model_id\n",
    "    model_name = 'model' + '_' + str(model_id) + '.pth'  \n",
    "    \n",
    "    # Get test data\n",
    "    test_X = torch.from_numpy(test_X).float()\n",
    "    test_set = TensorDataset(test_X)\n",
    "    test_loader = DataLoader(test_set, batch_size=config.batch_size)\n",
    "\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda:0\" if config.use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    model = Net(config).to(device)\n",
    "    model.load_state_dict(torch.load(config.model_save_path + model_name))  # Load model parameters\n",
    "\n",
    "    # First define a tensor to save the prediction results\n",
    "    result = torch.Tensor().to(device)\n",
    "\n",
    "    # Forecasting process\n",
    "    model.eval()\n",
    "    hidden_predict = None\n",
    "    for _data in test_loader:\n",
    "        data_X = _data[0].to(device)\n",
    "        pred_X, hidden_predict = model(data_X, hidden_predict)\n",
    "        cur_pred = torch.squeeze(pred_X, dim=0)\n",
    "        result = torch.cat((result, cur_pred), dim=0)\n",
    "\n",
    "    return result.detach().cpu().numpy()  # First remove the gradient information. If you want to transfer it to the cpu on the gpu,\n",
    "                                          # finally return the numpy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ef31d",
   "metadata": {},
   "source": [
    "# ModelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a1383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:22.078650Z",
     "start_time": "2023-05-15T09:13:22.029398Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelSet:\n",
    "\n",
    "    # Data parameters\n",
    "\n",
    "    start_feature = 0\n",
    "    end_feature = 3\n",
    "    start_label = 3\n",
    "    end_label = 5\n",
    "    delay_day = 0             # Delay prediction by a few days, for example, use today’s data to predict tomorrow’s data, delay_day = 1\n",
    "                              # Here today's driving data is used to predict today's runoff, so delay_day = 0\n",
    "\n",
    "    # Division of training test set and prediction set\n",
    "    start_train_and_valid_date_relative = 0\n",
    "    end_train_and_valid_date_relative = 144     # 2000-01-01 to 2013-12-31 training, # 2014-01-01 to 2017-12-31 verification\n",
    "    \n",
    "    start_test_date_relative = 0\n",
    "    end_test_date_relative = 1212\n",
    "\n",
    "    # Network parameters\n",
    "    input_size = end_feature -start_feature\n",
    "    output_size = end_label -start_label\n",
    "\n",
    "    # some hyperparameters\n",
    "    hidden_size = 150\n",
    "    lstm_layers = 2\n",
    "    dropout_rate = 0.4\n",
    "    time_step = 50\n",
    "\n",
    "    # training parameters\n",
    "    do_train = True     # Whether to train\n",
    "    do_test = True          # Verify or not\n",
    "\n",
    "    shuffle_train_data = False\n",
    "    use_cuda = True\n",
    "\n",
    "    train_data_rate = 0.7\n",
    "    valid_data_rate = 1-train_data_rate\n",
    "\n",
    "    batch_size = 120\n",
    "    learning_rate = 0.001\n",
    "    epoch = 256\n",
    "    patience = 5                # How many epochs should be trained? Stop if the validation set does not improve.\n",
    "    random_seed = 666           # Random seeds, guaranteed to be reproducible\n",
    "\n",
    "    # path parameters\n",
    "    model_save_path = '/group1/longjs/7-river-data/model_save/'\n",
    " \n",
    "    do_train_visualized = False\n",
    "    do_pred_save_to_file = False\n",
    "\n",
    "    # Create a directory\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5edb8",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf27c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:23.541737Z",
     "start_time": "2023-05-15T09:13:23.438447Z"
    }
   },
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, config , tair , prec , mass , runoff , et ):\n",
    "        self.config = config\n",
    "        self.tair = tair\n",
    "        self.prec = prec\n",
    "        self.mass = mass\n",
    "        self.runoff = runoff\n",
    "        self.et = et\n",
    "\n",
    "        self.data_start_train_and_valid_num = self.config.start_train_and_valid_date_relative\n",
    "        self.data_end_train_and_valid_num = self.config.end_train_and_valid_date_relative\n",
    "        self.data_start_pred_num = self.config.start_test_date_relative\n",
    "        self.data_stop_pred_num = self.config.end_test_date_relative     # The above times are based on the time of the original file\n",
    "\n",
    "        self.train_and_valid_num_end = self.data_end_train_and_valid_num-self.data_start_train_and_valid_num\n",
    "        self.train_num_end = int(self.train_and_valid_num_end * self.config.train_data_rate)\n",
    "        self.pred_num_start = self.data_start_pred_num - self.data_start_train_and_valid_num\n",
    "        self.pred_num_end = self.data_stop_pred_num - self.data_start_train_and_valid_num    # all relative to the time of extracting the data set.\n",
    "\n",
    "        self.data_raw = pd.DataFrame({'tair':self.tair,\n",
    "                                     'prec':self.prec,\n",
    "                                     'mass':self.mass,\n",
    "                                     'runoff':self.runoff,\n",
    "                                     'et':self.et})    # Receive data and convert it\n",
    "                \n",
    "        self.data = self.data_raw[self.data_start_train_and_valid_num:self.data_stop_pred_num]      # Get the intercepted file\n",
    "                                                                                                    # Includes training test set and prediction set\n",
    "        self.mean = np.nanmean(self.data, axis=0)              # The mean and variance of the data\n",
    "        self.std = np.nanstd(self.data, axis=0)\n",
    "        self.norm_data = (self.data - self.mean)/self.std   # Normalization, de-dimensionalization\n",
    "        self.norm_data = self.norm_data.fillna(0)\n",
    "\n",
    "        self.start_num_in_test = 0      # Data from the first few days in the test set will be deleted because it is not enough for one time_step\n",
    "\n",
    "\n",
    "    def get_train_and_valid_data(self):\n",
    "        feature_data = self.norm_data.iloc[:self.train_num_end,self.config.start_feature:self.config.end_feature]\n",
    "        label_data = self.norm_data.iloc[self.config.delay_day : self.config.delay_day + self.train_num_end,\n",
    "                                    self.config.start_label:self.config.end_label]    \n",
    "        # Use the data delayed a few days as the label. The default is the runoff data of the day.\n",
    "        \n",
    "        train_x = [feature_data[i:i+self.config.time_step] for i in range(self.train_num_end-self.config.time_step)]\n",
    "        train_y = [label_data[i:i+self.config.time_step] for i in range(self.train_num_end-self.config.time_step)]\n",
    "\n",
    "        train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=self.config.valid_data_rate,\n",
    "                                                              random_state=self.config.random_seed,\n",
    "                                                              shuffle=self.config.shuffle_train_data)   # Divide training and validation sets\n",
    "\n",
    "        return train_x, valid_x, train_y, valid_y\n",
    "\n",
    "    def get_test_data(self):\n",
    "        feature_data = self.norm_data.iloc[self.pred_num_start:self.pred_num_end,self.config.start_feature:self.config.end_feature]\n",
    "        sample_interval = min(feature_data.shape[0], self.config.time_step)     # Prevent time_step from being larger than the number of test sets\n",
    "        self.start_num_in_test = feature_data.shape[0] % sample_interval  # There is not enough data these days for one sample_interval\n",
    "        time_step_size = feature_data.shape[0] // sample_interval\n",
    "\n",
    "        # In the test data, each time_step row of data will be used as a sample, and the two samples are staggered by time_step rows.\n",
    "        test_x = [feature_data[self.start_num_in_test+i*sample_interval : self.start_num_in_test+(i+1)*sample_interval]\n",
    "                   for i in range(time_step_size)]\n",
    "        return np.array(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08052bd6",
   "metadata": {},
   "source": [
    "# rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39cfb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:24.987646Z",
     "start_time": "2023-05-15T09:13:24.948406Z"
    }
   },
   "outputs": [],
   "source": [
    "def rescale_and_evaluate(config: ModelSet, origin_data: Data,  predict_norm_data: np.ndarray):\n",
    "\n",
    "    predict_norm_data_reshape = predict_norm_data.reshape((-1,config.output_size))   # Become two-dimensional (time, prediction items, such as runoff, etc.)\n",
    "    predict_data = predict_norm_data_reshape * origin_data.std[config.start_label:config.end_label] + \\\n",
    "                   origin_data.mean[config.start_label:config.end_label]   # Restore data by saved mean and variance\n",
    "    \n",
    "    runoff = []\n",
    "    et = []\n",
    "    for i in range(len(predict_data)):\n",
    "        runoff = np.append(runoff,predict_data[i][0])\n",
    "        et = np.append(et,predict_data[i][1])\n",
    "    \n",
    "    if config.do_pred_save_to_file:\n",
    "        pd.DataFrame(predict_data).to_csv(config.pred_save_path+'predict'+str(model_id)+'.csv') # save csv file\n",
    "    \n",
    "    return runoff,et"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c71f1",
   "metadata": {},
   "source": [
    "# single_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517b3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:32.184726Z",
     "start_time": "2023-05-15T09:13:32.178043Z"
    }
   },
   "outputs": [],
   "source": [
    "defective_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548db430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:33.336706Z",
     "start_time": "2023-05-15T09:13:33.273898Z"
    }
   },
   "outputs": [],
   "source": [
    "def nse_value(s, o):\n",
    "    nse = he.evaluator(he.nse, s, o)\n",
    "    return nse\n",
    "\n",
    "def main(config, tair , prec , mass , runoff , et):\n",
    "    np.random.seed(config.random_seed)  # Set random seeds to ensure reproducibility\n",
    "    data_gainer = Data(config, tair , prec , mass , runoff , et)\n",
    "    \n",
    "    global defective_id\n",
    "    global model_id\n",
    "    \n",
    "    nse_all_finall_et = -100\n",
    "    nse_all_finall_runoff = -100\n",
    "    nse_target_et = 0.6\n",
    "    nse_target_runoff = 0.4\n",
    "    nse_count = 0\n",
    "    nse_count_all = 10\n",
    "    loop_count = 0\n",
    "  \n",
    "    while ((nse_all_finall_et < nse_target_et) and (nse_all_finall_runoff < nse_target_runoff)) and (nse_count<nse_count_all): \n",
    "\n",
    "        if config.do_train:\n",
    "            train_X, valid_X, train_Y, valid_Y = data_gainer.get_train_and_valid_data()\n",
    "            train(config, [train_X, train_Y, valid_X, valid_Y])\n",
    "\n",
    "        if config.do_test:\n",
    "            test_X = data_gainer.get_test_data()\n",
    "            pred_result = predict(config, test_X)      # The output here is the unrestored normalized prediction data.\n",
    "            predict_temp_runoff,predict_temp_et = rescale_and_evaluate(config, data_gainer, pred_result)\n",
    "\n",
    "        nse_all_temp_runoff = nse_value(predict_temp_runoff[0:192],data_gainer.runoff[12:204])\n",
    "        nse_train_temp_runoff = nse_value(predict_temp_runoff[0:132],data_gainer.runoff[12:144])\n",
    "        nse_test_temp_runoff = nse_value(predict_temp_runoff[132:192],data_gainer.runoff[144:204])\n",
    "                \n",
    "        nse_all_temp_et = nse_value(predict_temp_et[0:192],data_gainer.et[12:204])\n",
    "        nse_train_temp_et = nse_value(predict_temp_et[0:132],data_gainer.et[12:144])\n",
    "        nse_test_temp_et = nse_value(predict_temp_et[132:192],data_gainer.et[144:204])\n",
    "        \n",
    "        \n",
    "        if loop_count == 0:\n",
    "            predict_finall_et = predict_temp_et\n",
    "            predict_finall_runoff = predict_temp_runoff  \n",
    "\n",
    "            nse_all_finall_runoff = nse_all_temp_runoff\n",
    "            nse_train_finall_runoff = nse_train_temp_runoff\n",
    "            nse_test_finall_runoff = nse_test_temp_runoff\n",
    "                        \n",
    "            nse_all_finall_et = nse_all_temp_et\n",
    "            nse_train_finall_et = nse_train_temp_et\n",
    "            nse_test_finall_et = nse_test_temp_et\n",
    "            \n",
    "        else:\n",
    "            if nse_all_finall_et < nse_all_temp_et:\n",
    "                predict_finall_et = predict_temp_et\n",
    "                predict_finall_runoff = predict_temp_runoff\n",
    "\n",
    "                nse_all_finall_runoff = nse_all_temp_runoff\n",
    "                nse_train_finall_runoff = nse_train_temp_runoff\n",
    "                nse_test_finall_runoff = nse_test_temp_runoff\n",
    "                                \n",
    "                nse_all_finall_et = nse_all_temp_et\n",
    "                nse_train_finall_et = nse_train_temp_et\n",
    "                nse_test_finall_et = nse_test_temp_et\n",
    "\n",
    "                nse_count = 0\n",
    "                \n",
    "        loop_count += 1\n",
    "        nse_count += 1\n",
    "        \n",
    "        print('now loop = {} , Runoff: temp nse_all = {:.3f} , curren nse_all = {:.3f}, nse_train = {:.3f}, nse_test = {:.3f}'\n",
    "              .format(loop_count , nse_all_temp_runoff[0] , nse_all_finall_runoff[0] , nse_train_finall_runoff[0] , nse_test_finall_runoff[0]))    \n",
    "        print('                   ET: temp nse_all = {:.3f} , curren nse_all = {:.3f}, nse_train = {:.3f}, nse_test = {:.3f}'\n",
    "              .format(nse_all_temp_et[0] , nse_all_finall_et[0] , nse_train_finall_et[0] , nse_test_finall_et[0]))    \n",
    "        print(' ')\n",
    "        \n",
    "    if nse_count > 9:\n",
    "        defective_point = 1\n",
    "        defective_id.append(model_id)\n",
    "    else:\n",
    "        defective_point = 0\n",
    "        \n",
    "    return predict_finall_runoff,predict_finall_et,nse_all_finall_runoff,nse_train_finall_runoff,nse_test_finall_runoff,nse_all_finall_et,nse_train_finall_et,nse_test_finall_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879f99a",
   "metadata": {},
   "source": [
    "# single_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a3486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:48.143915Z",
     "start_time": "2023-05-15T09:13:48.106561Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_var_et = []\n",
    "predict_var_runoff = []\n",
    "def single_func(tair , prec , mass , runoff , et):\n",
    "    get_model_id()   # Counter +1\n",
    "    global model_id\n",
    "    global predict_var_et\n",
    "    global predict_var_runoff\n",
    "    \n",
    "    if np.isnan(runoff[0]):\n",
    "        predict_finall_et = np.full(1200,np.nan)\n",
    "        predict_finall_runoff = np.full(1200,np.nan)\n",
    "        \n",
    "        nse_all_runoff = np.nan\n",
    "        nse_train_runoff = np.nan\n",
    "        nse_test_runoff = np.nan\n",
    "                \n",
    "        nse_all_et = np.nan\n",
    "        nse_train_et = np.nan\n",
    "        nse_test_et = np.nan\n",
    "\n",
    "        defective_point = np.nan\n",
    "    else:\n",
    "        predict_finall_runoff,predict_finall_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = main(ModelSet(), tair , prec , mass , runoff , et)    \n",
    "        print('finish run model_{}, runoff nse = {:.3f} , et nse = {:.3f}'.format(model_id,nse_all_runoff[0],nse_all_et[0]))\n",
    "        print(' ')\n",
    "        print('{:=^100s}'.format('Next'))\n",
    "        print(' ')\n",
    "    \n",
    "    predict_var_runoff = np.append(predict_var_runoff,predict_finall_runoff)\n",
    "    predict_var_et = np.append(predict_var_et,predict_finall_et)\n",
    "    \n",
    "    return nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a8539",
   "metadata": {},
   "source": [
    "# apply_ufunc - Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96908406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:36.018995Z",
     "start_time": "2023-05-15T09:13:35.960935Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_func():\n",
    "    \n",
    "    # Read grid data\n",
    "    data_path = '/group1/longjs/7-river-data/'\n",
    "    tair = xr.open_dataset(data_path+'tair/'+'UKESM1_0_LL_SSP245_tas_1985_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    prec = xr.open_dataset(data_path+'prcp/'+'UKESM1_0_LL_SSP245_tp_1985_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    mass = xr.open_dataset(data_path+'mass/'+'massbaltot_RCP45_2000_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    runoff = xr.open_dataset(data_path+'ET-Runoff/'+'runoff_full_time_v4.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    et = xr.open_dataset(data_path+'ET-Runoff/'+'evaporation_full_time_v3.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    \n",
    "    # Calculation\n",
    "    nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = xr.apply_ufunc(single_func,\n",
    "                          tair['tair'],prec['prcp'],mass['massbaltot'],runoff['runoff'],et['evaporation'],\n",
    "                          input_core_dims=[['time'],['time'],['time'],['time'],['time']],\n",
    "                          output_core_dims=[[],[],[],[],[],[],[]],\n",
    "                          vectorize=True)\n",
    "    \n",
    "    # save\n",
    "    nse_all_runoff.name = 'nse_all'\n",
    "    nse_train_runoff.name = 'nse_train'\n",
    "    nse_test_runoff.name = 'nse_test'\n",
    "    \n",
    "    nse_all_et.name = 'nse_all'\n",
    "    nse_train_et.name = 'nse_train'\n",
    "    nse_test_et.name = 'nse_test'\n",
    "    \n",
    "    defective_point.name = 'defective_point'\n",
    "    \n",
    "    nse_all_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_all_runoff.nc')\n",
    "    nse_train_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_train_runoff.nc')\n",
    "    nse_test_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_test_runoff.nc')\n",
    "       \n",
    "    nse_all_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_all_et.nc')\n",
    "    nse_train_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_train_et.nc')\n",
    "    nse_test_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_test_et.nc')\n",
    "\n",
    "    \n",
    "    defective_point.to_netcdf(data_path+'result/'+'ukesm1_245_defective_point_et.nc')\n",
    "\n",
    "    global predict_var_runoff\n",
    "    global predict_var_et\n",
    "\n",
    "    pred_runoff = xr.Dataset({'runoff':(['time','lat','lon'],predict_var_runoff.reshape((60,140,1200)).transpose(2,0,1))},\n",
    "                      coords={'lon':np.arange(70.125,105,0.25),\n",
    "                              'lat':np.arange(25.125,40,0.25),\n",
    "                              'time':pd.date_range('20010101','21001231',freq='M')})\n",
    "    pred_et = xr.Dataset({'evaporation':(['time','lat','lon'],predict_var_et.reshape((60,140,1200)).transpose(2,0,1))},\n",
    "                      coords={'lon':np.arange(70.125,105,0.25),\n",
    "                              'lat':np.arange(25.125,40,0.25),\n",
    "                              'time':pd.date_range('20010101','21001231',freq='M')})\n",
    "    \n",
    "    pred_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_predict_runoff.nc')\n",
    "    pred_et.to_netcdf(data_path+'result/'+'ukesm1_245_predict_et.nc')\n",
    " \n",
    "    return pred_runoff,pred_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4009c8",
   "metadata": {},
   "source": [
    "# model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71868c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:13:37.572808Z",
     "start_time": "2023-05-15T09:13:37.564798Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "def get_model_id():\n",
    "    global model_id\n",
    "    model_id += 1\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2269a",
   "metadata": {},
   "source": [
    "# all_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfa26d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:53:57.341241Z",
     "start_time": "2023-02-28T14:05:08.138823Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_runoff,pred_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = grid_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507a3dd",
   "metadata": {},
   "source": [
    "# simple analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a450575",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:45:40.326948Z",
     "start_time": "2023-02-28T13:45:40.024347Z"
    }
   },
   "outputs": [],
   "source": [
    "nse_train_et.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01104d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T12:36:38.621985Z",
     "start_time": "2023-02-28T12:36:38.600489Z"
    }
   },
   "outputs": [],
   "source": [
    "u = 2.5*df_data.describe()['nse_all']['75%'] -df_data.describe()['nse_all']['25%']\n",
    "d = 2.5*df_data.describe()['nse_all']['25%'] -1.5*df_data.describe()['nse_all']['75%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c9a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T11:53:47.279401Z",
     "start_time": "2023-02-03T11:53:47.272879Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(nse_all>u)\n",
    "np.sum(nse_all<d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e76be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:23.070588Z",
     "start_time": "2023-02-28T15:05:22.777480Z"
    }
   },
   "outputs": [],
   "source": [
    "defective_point.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4794d5b",
   "metadata": {},
   "source": [
    "# defective run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2af9d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:26.886931Z",
     "start_time": "2023-02-28T15:05:26.883393Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "defective_id_raw = copy.deepcopy(defective_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95a665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:27.429688Z",
     "start_time": "2023-02-28T15:05:27.426672Z"
    }
   },
   "outputs": [],
   "source": [
    "# defective_id_raw = copy.deepcopy(defective_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997ab42",
   "metadata": {},
   "source": [
    "## ModelSet_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bcf1f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:28.417587Z",
     "start_time": "2023-02-28T15:05:28.410273Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelSet_de:\n",
    "\n",
    "\n",
    "\n",
    "    start_feature = 1         \n",
    "    end_feature = 2\n",
    "    start_label = 3\n",
    "    end_label = 5\n",
    "    delay_day = 0            \n",
    "\n",
    "    start_train_and_valid_date_relative = 0\n",
    "    end_train_and_valid_date_relative = 144     \n",
    "    \n",
    "    start_test_date_relative = 0\n",
    "    end_test_date_relative = 1212\n",
    "\n",
    "\n",
    "    input_size = end_feature-start_feature\n",
    "    output_size = end_label-start_label\n",
    "\n",
    "\n",
    "    hidden_size = 50\n",
    "    lstm_layers = 2\n",
    "    dropout_rate = 0.4\n",
    "    time_step = 50\n",
    "\n",
    "    do_train = True    \n",
    "    do_test = True       \n",
    "\n",
    "    shuffle_train_data = False\n",
    "    use_cuda = True\n",
    "\n",
    "    train_data_rate = 0.7\n",
    "    valid_data_rate = 1-train_data_rate\n",
    "\n",
    "    batch_size = 24\n",
    "    learning_rate = 0.001\n",
    "    epoch = 256\n",
    "    patience = 5               \n",
    "    random_seed = 666           \n",
    "\n",
    "\n",
    "    model_save_path = '/group1/longjs/7-river-data/model_save/'\n",
    " \n",
    "    do_train_visualized = False\n",
    "    do_pred_save_to_file = False\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2b2bf",
   "metadata": {},
   "source": [
    "## single_main_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5406738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:29.458201Z",
     "start_time": "2023-02-28T15:05:29.449125Z"
    }
   },
   "outputs": [],
   "source": [
    "def nse_value(s, o):\n",
    "    nse = he.evaluator(he.nse, s, o)\n",
    "    return nse\n",
    "\n",
    "def predict_de(config, tair , prec , mass , runoff , et):\n",
    "    np.random.seed(config.random_seed) \n",
    "    data_gainer = Data(config, tair , prec , mass , runoff , et)\n",
    "  \n",
    "    test_X = data_gainer.get_test_data()\n",
    "    pred_result = predict(config, test_X)     \n",
    "    predict_finall_runoff,predict_finall_et = rescale_and_evaluate(config, data_gainer, pred_result)\n",
    "        \n",
    "    nse_all_finall_runoff = nse_value(predict_finall_runoff[0:192],data_gainer.runoff[12:204])\n",
    "    nse_train_finall_runoff = nse_value(predict_finall_runoff[0:132],data_gainer.runoff[12:144])\n",
    "    nse_test_finall_runoff = nse_value(predict_finall_runoff[132:192],data_gainer.runoff[144:204])\n",
    "    \n",
    "    nse_all_finall_et = nse_value(predict_finall_et[0:192],data_gainer.et[12:204])\n",
    "    nse_train_finall_et = nse_value(predict_finall_et[0:132],data_gainer.et[12:144])\n",
    "    nse_test_finall_et = nse_value(predict_finall_et[132:192],data_gainer.et[144:204])\n",
    "\n",
    "    print('Runoff: temp nse_all = {:.3f} , curren nse_all = {:.3f}, nse_train = {:.3f}, nse_test = {:.3f}'\n",
    "          .format(nse_all_finall_runoff[0] , nse_all_finall_runoff[0] , nse_train_finall_runoff[0] , nse_test_finall_runoff[0]))    \n",
    "    print('    ET: temp nse_all = {:.3f} , curren nse_all = {:.3f}, nse_train = {:.3f}, nse_test = {:.3f}'\n",
    "          .format(nse_all_finall_et[0] , nse_all_finall_et[0] , nse_train_finall_et[0] , nse_test_finall_et[0]))    \n",
    "    print(' ')\n",
    "    \n",
    "    defective_point = 0\n",
    "         \n",
    "    return predict_finall_runoff,predict_finall_et,nse_all_finall_runoff,nse_train_finall_runoff,nse_test_finall_runoff,nse_all_finall_et,nse_train_finall_et,nse_test_finall_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cecf436",
   "metadata": {},
   "source": [
    "## single_func_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3780b93d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:31.496798Z",
     "start_time": "2023-02-28T15:05:31.493964Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d73ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:32.063655Z",
     "start_time": "2023-02-28T15:05:32.060944Z"
    }
   },
   "outputs": [],
   "source": [
    "defective_id_run = copy.deepcopy(defective_id_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcb5dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:32.510709Z",
     "start_time": "2023-02-28T15:05:32.507539Z"
    }
   },
   "outputs": [],
   "source": [
    "defective_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e70ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:55:47.529207Z",
     "start_time": "2023-02-28T03:55:47.525899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use in cycles, do not run repeatedly\n",
    "defective_id_run = copy.deepcopy(defective_id)\n",
    "defective_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85166f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:35.178076Z",
     "start_time": "2023-02-28T15:05:35.173468Z"
    }
   },
   "outputs": [],
   "source": [
    "len(defective_id_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f7ffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:05:37.951402Z",
     "start_time": "2023-02-28T15:05:37.914573Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_var_et_de = []\n",
    "predict_var_runoff_de = []\n",
    "\n",
    "import copy\n",
    "def single_func_de(tair , prec , mass , runoff , et):\n",
    "    get_model_id()  \n",
    "    global model_id\n",
    "    global predict_var_et_de\n",
    "    global predict_var_runoff_de\n",
    "    global defective_id_run\n",
    "\n",
    "    \n",
    "    if model_id in defective_id_run:\n",
    "        predict_finall_runoff,predict_finall_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = main(ModelSet_de(), tair , prec , mass , runoff , et)  \n",
    "        print('Run model_{}, et nse = {:.3f} , runoff nse = {:.3f}'.format(model_id,nse_all_et[0],nse_all_runoff[0]))\n",
    "        print(' ')\n",
    "        print('{:=^100s}'.format('Run'))\n",
    "        print(' ')\n",
    "    else:\n",
    "        if np.isnan(runoff[0]):\n",
    "            predict_finall_et = np.full(1200,np.nan)\n",
    "            predict_finall_runoff = np.full(1200,np.nan)\n",
    "\n",
    "            nse_all_et = np.nan\n",
    "            nse_train_et = np.nan\n",
    "            nse_test_et = np.nan\n",
    "\n",
    "            nse_all_runoff = np.nan\n",
    "            nse_train_runoff = np.nan\n",
    "            nse_test_runoff = np.nan\n",
    "\n",
    "            defective_point = np.nan\n",
    "        else: \n",
    "            predict_finall_runoff,predict_finall_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = predict_de(ModelSet(), tair , prec , mass , runoff , et)  \n",
    "            print('Predict model_{}, et nse = {:.3f} , runoff nse = {:.3f}'.format(model_id,nse_all_et[0],nse_all_runoff[0]))\n",
    "            print(' ')\n",
    "            print('{:=^100s}'.format('Predict'))\n",
    "            print(' ')\n",
    "    \n",
    "    predict_var_et_de = np.append(predict_var_et_de,predict_finall_et)\n",
    "    predict_var_runoff_de = np.append(predict_var_runoff_de,predict_finall_runoff)\n",
    "    \n",
    "    return nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0f0ff",
   "metadata": {},
   "source": [
    "## apply_ufunc_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7ef68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:06:11.621759Z",
     "start_time": "2023-02-28T15:06:11.608812Z"
    }
   },
   "outputs": [],
   "source": [
    "def grid_func_de():\n",
    "    \n",
    "\n",
    "    data_path = '/group1/longjs/7-river-data/'\n",
    "    tair = xr.open_dataset(data_path+'tair/'+'UKESM1_0_LL_SSP245_tas_1985_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    prec = xr.open_dataset(data_path+'prcp/'+'UKESM1_0_LL_SSP245_tp_1985_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    mass = xr.open_dataset(data_path+'mass/'+'massbaltot_RCP45_2000_2100.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    runoff = xr.open_dataset(data_path+'ET-Runoff/'+'runoff_full_time_v3.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    et = xr.open_dataset(data_path+'ET-Runoff/'+'evaporation_full_time_v3.nc').sel(time=slice('2000-01-01','2100-12-31'))\n",
    "    \n",
    "\n",
    "    nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = xr.apply_ufunc(single_func_de,\n",
    "                          tair['tair'],prec['prcp'],mass['massbaltot'],runoff['runoff'],et['evaporation'],\n",
    "                          input_core_dims=[['time'],['time'],['time'],['time'],['time']],\n",
    "                          output_core_dims=[[],[],[],[],[],[],[]],\n",
    "                          vectorize=True)\n",
    "    \n",
    "\n",
    "    nse_all_runoff.name = 'nse_all'\n",
    "    nse_train_runoff.name = 'nse_train'\n",
    "    nse_test_runoff.name = 'nse_test'\n",
    "    \n",
    "    nse_all_et.name = 'nse_all'\n",
    "    nse_train_et.name = 'nse_train'\n",
    "    nse_test_et.name = 'nse_test'\n",
    "    \n",
    "    defective_point.name = 'defective_point'\n",
    "    \n",
    "    nse_all_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_all_runoff_de.nc')\n",
    "    nse_train_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_train_runoff_de.nc')\n",
    "    nse_test_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_nse_test_runoff_de.nc')\n",
    "       \n",
    "    nse_all_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_all_et_de.nc')\n",
    "    nse_train_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_train_et_de.nc')\n",
    "    nse_test_et.to_netcdf(data_path+'result/'+'ukesm1_245_nse_test_et_de.nc')\n",
    "\n",
    "    \n",
    "    defective_point.to_netcdf(data_path+'result/'+'ukesm1_245_defective_point_et_de.nc')\n",
    "\n",
    "    global predict_var_runoff\n",
    "    global predict_var_et\n",
    "\n",
    "    pred_runoff = xr.Dataset({'runoff':(['time','lat','lon'],predict_var_runoff.reshape((60,140,1200)).transpose(2,0,1))},\n",
    "                      coords={'lon':np.arange(70.125,105,0.25),\n",
    "                              'lat':np.arange(25.125,40,0.25),\n",
    "                              'time':pd.date_range('20010101','21001231',freq='M')})\n",
    "    pred_et = xr.Dataset({'evaporation':(['time','lat','lon'],predict_var_et.reshape((60,140,1200)).transpose(2,0,1))},\n",
    "                      coords={'lon':np.arange(70.125,105,0.25),\n",
    "                              'lat':np.arange(25.125,40,0.25),\n",
    "                              'time':pd.date_range('20010101','21001231',freq='M')})\n",
    "    \n",
    "    pred_runoff.to_netcdf(data_path+'result/'+'ukesm1_245_predict_runoff_de.nc')\n",
    "    pred_et.to_netcdf(data_path+'result/'+'ukesm1_245_predict_et_de.nc')\n",
    " \n",
    "    return pred_runoff,pred_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29b79c",
   "metadata": {},
   "source": [
    "## model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88764e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:06:12.733218Z",
     "start_time": "2023-02-28T15:06:12.729815Z"
    }
   },
   "outputs": [],
   "source": [
    "model_id = 0\n",
    "def get_model_id():\n",
    "    global model_id\n",
    "    model_id += 1\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fc4ba",
   "metadata": {},
   "source": [
    "## all_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8aa328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T15:22:18.789107Z",
     "start_time": "2023-02-28T15:06:14.352584Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_runoff,pred_et,nse_all_runoff,nse_train_runoff,nse_test_runoff,nse_all_et,nse_train_et,nse_test_et,defective_point = grid_func_de()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940efc7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T01:17:30.557174Z",
     "start_time": "2023-03-01T01:17:30.550730Z"
    }
   },
   "outputs": [],
   "source": [
    "len(defective_id_run)\n",
    "len(defective_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4f84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "gpu-py3.10",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.047px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "774.094px",
    "left": "1472.38px",
    "right": "20px",
    "top": "140.938px",
    "width": "377.297px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
